<div align="center">

# fo-VQ-VAE <!-- omit in toc -->
[![ColabBadge]][notebook]
[![PaperBadge]][paper]  

</div>

Extened VQ-VAE: Conv-VQ-WaveRNN extended with explicit fo module

<!-- Auto-generated by "Markdown All in One" extension -->
- [Demo](#demo)
- [Usage](#usage)
  - [Install](#install)
  - [Train](#train)
  - [Inference](#inference)
- [Results](#results)
- [References](#references)

![Framework of extended VQVAE](https://github.com/nii-yamagishilab/Extended_VQVAE/blob/master/framework.png?raw=true)


## Demo
Official [demo page]. 

## Usage
### Install

install modules in requirement.txt

```bash
# pip install "torch==1.11.0" -q      # Based on your environment (validated with vX.YZ)
# pip install "torchaudio==0.11.0" -q # Based on your environment
pip install git+https://github.com/terepan/foVQVAE
```

### Train
Jump to ☞ [![ColabBadge]][notebook], then Run. That's all!  

For arguments, check [./fovqvae/config.py](https://github.com/terepan/foVQVAE/blob/main/fovqvae/config.py).  
For dataset, check [`speechcorpusy`](https://github.com/tarepan/speechcorpusy).  

#### Preprocessing
1. extract F0. (We used [crepe](https://github.com/marl/crepe) to extract F0.  )
2. F0 and Wavefrom Alignment
3. converting F0 and waveform into *.npy format.

#### Train
Please use ./run.sh  when train an extended vavae model.

Or you can use python3 train.py -m [model type]. The -m option can be used to tell the the script to train a different model.

[model type] can be:
- 'vqvae': Train original VQVAE
- 'wavernn': train an WaveRNN model
- 'vcf0': extended VQVAE with F0 encoder

Please modify sampling rate and other parameters in [config.py](https://github.com/nii-yamagishilab/Extended_VQVAE/blob/master/config.py) before training.  
For Multi-gpu parallel training, please see [multi_gpu_wavernn.py](https://github.com/nii-yamagishilab/Extended_VQVAE/blob/master/multi_gpu_wavernn.py)  

### Inference
Both CLI and Python supported.  
For detail, jump to ☞ [![ColabBadge]][notebook] and check it.  

## Results
### Sample <!-- omit in toc -->
[Demo](#demo)

### Performance <!-- omit in toc -->
- training
  - x.x [iter/sec] @ NVIDIA X0 on Google Colaboratory (AMP+)
  - take about y days for whole training
- inference
  - z.z [sec/sample] @ xx

### Pre-Trained models
original repo:  

> We have Japanese or Chinese trained models for both original VQVAE and extended VQVAE.
> If you’re interested in using our pre-trained models for research purpose, please contact the zhaoyi[email mark]nii.ac.jp.

## References
### Original paper <!-- omit in toc -->
[![PaperBadge]][paper]  
<!-- Generated with the tool -> https://arxiv2bibtex.org/?q=2005.07884&format=bibtex -->
```
@misc{2005.07884,
Author = {Yi Zhao and Haoyu Li and Cheng-I Lai and Jennifer Williams and Erica Cooper and Junichi Yamagishi},
Title = {Improved Prosody from Learned F0 Codebook Representations for VQ-VAE Speech Waveform Reconstruction},
Year = {2020},
Eprint = {arXiv:2005.07884},
}
```

### Acknowlegements <!-- omit in toc -->
- [mkotha/WaveRNN](https://github.com/mkotha/WaveRNN)


[ColabBadge]:https://colab.research.google.com/assets/colab-badge.svg

[paper]:https://arxiv.org/abs/2005.07884
[PaperBadge]:https://img.shields.io/badge/paper-arxiv.2005.07884-B31B1B.svg
[notebook]:https://colab.research.google.com/github/tarepan/foVQVAE/blob/main/fovqvae.ipynb
[demo page]:https://nii-yamagishilab.github.io/yi-demo/interspeech-2020/index.html